
import os
import shutil
import json
from pathlib import Path
from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip
from pyspark.sql.functions import col, explode

# Importar el cliente MeteoGalicia
from src.clients.meteogalicia_client import MeteoGaliciaClient

# Crear SparkSession con Delta
builder = SparkSession.builder \
    .appName("Ingesta_MeteoGalicia") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
spark = configure_spark_with_delta_pip(builder).getOrCreate()

# Ruta base del proyecto (2 niveles arriba de este script)
base_path = Path(__file__).resolve().parents[2]

# Ruta del output en Delta Lake
delta_output_path = base_path / "data" / "delta" / "meteogalicia_estaciones"

# Ruta temporal para el JSON descargado
json_temp_path = base_path / "data" / "raw" / "meteogalicia" / "METEOGALICIA_stations_temp.json"

# FunciÃ³n para borrar el directorio Delta si ya existe
def reset_output(path: Path):
    if path.exists() and path.is_dir():
        print(f"ðŸ§¹ Borrando carpeta existente: {path}")
        shutil.rmtree(path)
    else:
        print(f"ðŸ“‚ No existe aÃºn la carpeta Delta: {path}")


def descargar_meteogalicia_json(json_path):
    client = MeteoGaliciaClient()
    # Guardar el JSON en la ruta temporal especificada
    client.save_json = lambda name, data, include_date=False: json_path.write_text(
        json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    client.get_stations()

# Borrar output anterior si existe
reset_output(delta_output_path)

# Descargar el JSON dinÃ¡micamente
descargar_meteogalicia_json(json_temp_path)

# Cargar JSON multi-lÃ­nea
raw_df = spark.read.option("multiLine", True).json(str(json_temp_path))
raw_df.printSchema()
raw_df.show(truncate=False)

# Explodear las features
features_df = raw_df.select(explode(col("features")).alias("feature"))

# Aplanar campos relevantes
flat_df = features_df.select(
    col("feature.attributes.idEstacion").alias("idEstacion"),
    col("feature.attributes.Estacion").alias("nombre"),
    col("feature.attributes.Concello").alias("concello"),
    col("feature.attributes.idConcello").alias("idConcello"),
    col("feature.attributes.provincia").alias("provincia"),
    col("feature.geometry.x").alias("x_coord"),
    col("feature.geometry.y").alias("y_coord")
)

# Guardar en Delta Lake
flat_df.write.format("delta").mode("overwrite").save(str(delta_output_path))
print(f"âœ… Datos guardados en formato Delta: {delta_output_path}")

# Leer los datos guardados para visualizaciÃ³n
df = spark.read.format("delta").load(str(delta_output_path))
df.show()

# Eliminar el JSON temporal
if json_temp_path.exists():
    json_temp_path.unlink()
# NECESARIO JAVA 11

from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip
from pyspark.sql.functions import col, explode, lower, trim, count, isnan
import shutil
from pathlib import Path
import json

# Importar el cliente MeteoGalicia
from src.clients.meteogalicia_client import MeteoGaliciaClient

# 🔧 Crear SparkSession con soporte Delta Lake
builder = SparkSession.builder \
    .appName("Trusted_Zone_MeteoGalicia") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
spark = configure_spark_with_delta_pip(builder).getOrCreate()

# 📁 Definir rutas de entrada (raw) y salida (trusted)
base_path = Path(__file__).resolve().parents[2]
json_temp_path = base_path / "data" / "raw" / "meteogalicia" / "METEOGALICIA_stations_temp.json"
trusted_path = base_path / "data" / "trusted" / "meteogalicia_estaciones"

# Descargar el JSON dinámicamente desde la API
def descargar_meteogalicia_json(json_path):
    client = MeteoGaliciaClient()
    client.save_json = lambda name, data, include_date=False: json_path.write_text(
        json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    client.get_stations()

descargar_meteogalicia_json(json_temp_path)

# 🧹 Eliminar carpeta de salida si ya existe para evitar duplicidad de logs y parquet
if trusted_path.exists():
    shutil.rmtree(trusted_path)

# 📥 Cargar JSON con formato GeoJSON (multilínea)
raw_df = spark.read.option("multiLine", True).json(str(json_temp_path))

# 🔍 Explodear la lista de features para procesarlas individualmente
features_df = raw_df.select(explode(col("features")).alias("feature"))

# ✨ Función para limpieza básica de texto: minúsculas y sin espacios
def clean_text(colname):
    return lower(trim(col(colname)))

# 🧼 Aplanamiento del JSON + limpieza de columnas clave
flat_df = features_df.select(
    col("feature.attributes.idEstacion").cast("int").alias("idEstacion"),
    clean_text("feature.attributes.Estacion").alias("nombre"),
    clean_text("feature.attributes.Concello").alias("concello"),
    col("feature.attributes.idConcello").cast("int").alias("idConcello"),
    clean_text("feature.attributes.provincia").alias("provincia"),
    col("feature.geometry.x").cast("double").alias("x_coord"),
    col("feature.geometry.y").cast("double").alias("y_coord")
)

# 🧪 Mostrar número de registros antes de limpieza
print("📊 Registros antes de limpieza:", flat_df.count())

# ✅ LIMPIEZA: eliminar duplicados por clave primaria
flat_df = flat_df.dropDuplicates(["idEstacion"])

# ✅ LIMPIEZA: eliminar registros con nulos en columnas clave
flat_df = flat_df.dropna(subset=["idEstacion", "x_coord", "y_coord"])

# 🔍 Función para validar duplicados
def assert_no_duplicates(df, key_col: str):
    """Verifica que no haya duplicados en la columna clave."""
    dup_count = (
        df.groupBy(key_col)
          .agg(count("*").alias("n"))
          .filter(col("n") > 1)
          .count()
    )
    if dup_count > 0:
        raise ValueError(f"❌ Se encontraron {dup_count} claves duplicadas en '{key_col}'")
    else:
        print(f"✅ No hay duplicados en '{key_col}' — validación pasada.")

# 🔍 Función para validar valores nulos
def assert_no_nulls(df, cols: list):
    """Verifica que no haya nulos en las columnas clave."""
    for colname in cols:
        nulls = df.filter(col(colname).isNull() | isnan(col(colname))).count()
        if nulls > 0:
            raise ValueError(f"❌ Se encontraron {nulls} valores nulos en la columna '{colname}'")
        else:
            print(f"✅ Sin nulos en '{colname}' — validación pasada.")

# 🧪 Validaciones finales
assert_no_duplicates(flat_df, "idEstacion")
assert_no_nulls(flat_df, ["idEstacion", "x_coord", "y_coord"])

# 💾 Guardar en formato Delta en la zona "trusted"
flat_df.write.format("delta").mode("overwrite").save(str(trusted_path))
print(f"✅ Trusted zone lista en: {trusted_path}")

# 🗑️ Eliminar el JSON temporal
if json_temp_path.exists():
    json_temp_path.unlink()